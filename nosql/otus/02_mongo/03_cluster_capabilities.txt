
//////////////////////////
//1. Варианты репликации//
//////////////////////////
//Зачем нужда репликация
 - Быстрое переключение на бэкап сервер
 - Горизонтальное масштабирование
 - Чтобы снимать бэкап с реплики, не нагружая мастер
 - Геораспределение нагрузки

//Виды репликации
1. Replicaset
2. Master-slave (до версии 3.6)

//Replicaset
 - запросы на чтение и на запись идут на Primary
 - с Primary на Secondary идут репликации
 - каждая нода опрашивает каждую ноду живы они или нет (Heartbeat)
 - если primary падает, происходит перевыбор и одна из нод становится primary

//Arbiter
 - Secondary сервер можно заменить на Arbiter
 - Arbiter выбирает новый Primary в результате голосования
 - Данных арбитр не содержит
 - может сделать репликасет более легковесным

//Особенности репликации
 - репликация асинхронная
 - арбитр не содержит данных и поэтому очень легковесный
 - primary может стать secondary и наоборот, а
арбитр не может стать ни primary, ни secondary
 - максимальное количество реплик 50, из них голосовать разрешено только 7
 - запуск арбитра на той же машине, что и реплика не рекомендован,
т.к. умрет виртуалка - умрет и реплика и арбитр
 - обычно репликасеты встречаются - 3-5 серверов

//Практика
//Можно настроить данный РепликаСет несколькими способами:
 - Использовать 1 сервер и запустить 3 экземпляра самой mongoDB.
 - Использовать 3 сервера с mongoDB.
//Итак, для правильно работы ReplicaSet необходимо 3 запущенных экземпляра или сервера с монгой:
 - Одна нода будет выступать как арбитр и не будет принимать на себя никакие данные. Его
работа, заключается в том, чтобы выбрать того, кто будет сервером PRIMARY.
 - Один сервер выступает в качестве PRIMARY сервера.
 - Один сервер выступает в качестве SECONDARY сервера.
 - для доступа к экземпляру mongo из сети (не localhost) необходимо настроить встроенный файрвол и
 указать, например, --bind_ip_all
 - https://docs.mongodb.com/manual/core/security-mongodb-configuration/

//Проблемы, которые могут возникнуть при чтении с реплики:
 - так как запись асинхронная, она может быть уже сделана
на primary, но не доехать до secondary и мы прочитаем
старые данные с secondary
 - отсюда же вытекает что записав данные на основной
сервер, мы не можем быть уверены, когда остальные
получат эти данные
 - опять же мы можем установить приоритет на запись или чтение
 - ну и самое веселое, когда падает основной сервер и
происходят выборы (неск секунд не будет проходить чтение)

//Варианты сделать доступной реплику для чтения:
(По умолчанию чтение напрямую с secondary запрещено
Можно зайти на ноду, выполнить rs.slaveOk() или rs.secondaryOk(),
и secondary будет обрабатывать find)
 - db.slaveOk()
 - указывать в строке подключения драйвера нужные параметры
https://docs.mongodb.com/manual/reference/connection-string/
 - Более точечное использование
db.collection.find({}).readPref( "secondary", [ { "region": "South" } ] )
при создании реплики указываем облако тегов для каждого экземпляра,
потом обращаемся по нему
https://docs.mongodb.com/manual/reference/method/cursor.readPref/

//Режимы
Read Preference Mode / Description
//1. primary
Default mode. All operations read from the current replica set primary.
Multi-document transactions that contain read operations must use read preference primary. All operations in a given
transaction must route to the same member.
//2. primaryPreferred
In most situations, operations read from the primary but if it is unavailable, operations read from secondary members.
//3. secondary
All operations read from the secondary members of the replica set.
//4. secondaryPreferred
In most situations, operations read from secondary members but if no secondary members are available, operations read from
the primary.
//5. nearest
Operations read from member of the replica set with the least network latency, irrespective of the member’s type.

////////////////////////
//2. Концепция кворума//
////////////////////////
//Особенность для 3 нод
//при конфигурации Primary-Secondary-Arbiter (P-S-A) и writeConcern : majority
 - необходимо 2 ноды для согласования записи данных, соответственно при
падении Primary или Secondary у нас останется только чтение и
неконсистентная запись с “w:1” - подтверждение еще с 1 ноды нужно %(

//при конфигурации Primary-Secondary-Secondary (P-S-S) и writeConcern : majority
 - при падении 1 ноды запись останется работать
https://docs.mongodb.com/manual/reference/write-concern/#calculating-majority-count

//Визуализация выбора нового primary
https://raft.github.io/

//////////////////////
//3. Масштабирование//
//////////////////////
//Что делать, когда все тормозит и репликасет не спасает
1. отключаем подтверждение записи primary большинством голосов {w:1, j:true}
2. отключаем журналирование {w:1, j:false}
3. отключаем любые проверки {w:0, j:false}
4. что дальше?
https://docs.mongodb.com/manual/reference/write-concern/
https://habr.com/ru/post/335698/

///////////////////
//4. Шардирование//
///////////////////
 - Есть коллекция, мы ее хотим как-то раскидать по шардам (репликасетам).
 - Для этого MongoDB делит коллекцию на чанки с помощью ключа
шардирования, пытаясь поделить их в пространстве шард-ключа на равные кусочки.
 - Дальше включается балансировщик, который старательно
раскладывает эти чанки по шардам в кластере.
Причем балансировщику все равно, сколько эти чанки весят и сколько в них
документов, так как балансировка идет в чанках поштучно.

//Минусы:
 - С шардированием обязательно будет усложняться код — во многие места придется
протащить ключ шардирования. Это не всегда удобно, и не всегда возможно.
Некоторые запросы пойдут либо broadcast, либо multicast, что тоже не добавляет
масштабируемости. Подходите к выбору ключа по которому пойдет шардирование
аккуратнее.
 - В шардированных коллекциях ломается операция count. Она начинает возвращать
число больше, чем в действительности — может соврать в 2 раза. Причина лежит в
процессе балансировки, когда документы переливаются с одного шарда на другой.
Когда документы перелились на соседний шард, а на исходном еще не удалились —
count их все равно посчитает.
 - Шардированный кластер гораздо тяжелее в администрировании. Девопсы
перестанут с вами здороваться, потому что процедура снятия бэкапа становится
радикально сложнее. Необходимо большая автоматизация работы.

//Проблемы: при включении шардинга у нас есть 2 основные группы проблем.
//1. Шардинг коллекции не возможен, если:
 - требуется обновление полей, входящих в ключ шардирования;
 - на коллекции есть несколько уникальных ключей;
 - под результат запроса findAndModify попадают данные на разных шардах.
//2. Несбалансированная нагрузка по шардам, если:
 - слабая селективность ключа шардирования;
 - запросы без значений ключа шардинга.

//Как работает
В схеме мы можем видеть config сервер, его отличие от mongod в том, что он не
обрабатывает клиентские запросы, а является хранилищем метаданных — он знает
физические адреса всех chunk-ов, знает какой chunk, на каком шарде искать и какой
диапазон у того или иного шарда. Все эти данные он хранит в специально
отведенном месте — config database.
//В данной схеме также присутствует роутер запросов — mongos, на него возлагаются
следующие задачи:
1. Кэширование данных, хранимых на config сервере
2. Роутинг запросов чтения и записи от драйвера — маршрутизация запросов от
приложений на нужные шарды, mongos точно знает где физически находится тот
или иной чанк
3. Запуск фонового процесса “Балансер”

//Балансировка шардов
Шардируем, запускаем, и внезапно узнаем, что балансировка не бесплатна.
Балансировка проходит несколько стадий. Балансировщик выбирает чанки и шарды, откуда и куда
будет переносить. Дальнейшая работа идет в две фазы: сначала документы копируются с источника
в цель, а потом документы, которые были скопированы, удаляются.
Шард у нас перегружен, в нем лежат все коллекции, но первая часть операции для него легкая. А вот
вторая — удаление — совсем неприятная, потому что уложит на лопатки шард и так страдающий под
нагрузкой.
Проблема усугубляется тем, что если мы балансируем много чанков, например, тысячи, то с
настройками по умолчанию все эти чанки сначала копируются, а потом приходит удалятор и начинает
их скопом удалять. В этот момент на процедуру уже не повлиять и приходится только грустно
наблюдать за происходящим.
Поэтому если вы подходите к тому, чтобы шардировать перегруженный кластер, вам нужно
планировать, так как балансировка занимает время. Желательно это время брать не в прайм-тайм, а
в периоды низкой нагрузки. Балансировщик — отключаемая запчасть. Можно подойти к первичной
балансировке в ручном режиме, отключать балансировщик в прайм-тайм, и включать, когда нагрузка
снизилась, чтобы позволить себе больше.

//////////////////////////////////////////
//5. Правильный выбор ключа шардирования//
//////////////////////////////////////////

У хорошего ключа несколько параметров: высокий cardinality, немутабельность и он
хорошо ложится в частые запросы.
Важно заметить, что при достижении каким-либо чанком неделимого диапазона,
например (44, 45], деление происходить не будет и чанк будет расти свыше chunksize.
Поэтому следует внимательно выбирать shard key, чтобы это была как можно наиболее
случайная величина. Например, если нам надо заполнить БД всеми людьми на планете,
то удачными выборами shard key были бы номер телефона, идентификационный
налоговый номер, почтовый индекс. Неудачными — имя, город проживания.
Естественный выбор ключа шардирования — это первичный ключ — поле id. Если
вам подходит для шардирования поле id, то лучше прямо по нему и шардировать. Это
отличный выбор — у него и cardinality хороший, он и немутабельный, но а насколько
хорошо ложится в частые запросы — это ваша бизнес-специфика.